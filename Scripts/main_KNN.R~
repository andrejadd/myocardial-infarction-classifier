
require(class)

##
## K-nearest neightbours classification
##


##
## Calculates average test MSE for each parameter k = (1,...,n-1) using LOOCV.
## Returns 'k' with lowest MSE.
##
## Note that for classification problems, i.e. for discrete outcomes, the Brier Score (BS) is the
##  proper scoring function. However, for binary outcomes, the BS is equivalent to to the MSE:
##
##     BS = MSE = 1/n * sum_i=1^n (y_i - f(x_i))^2
##
## where f(x_i) is the probability of the forecast. 
##
main_KNN_bestk <- function(heart_data, OUT_DIR, DATA_SETUP_TEST) {
                              


    results = list()


    ##
    ## Scale data : Important for KNN whenever variables ranges differ greatly.
    ##              KNN uses a distance measure and needs variables in similar intervals.
    ##
    
    data.sc = data.frame(scale(heart_data))
    data.sc[,1] = heart_data$Type
      
    nr_samples = nrow(data.sc)

        
    ##
    ## Run KNN with LOOCV and varying parameters k = (1,...,n-1)
    ## Calculate MSE_test for each iteration of LOOCV, take average at the end
    ## and select k with lowest mean MSE_test.
    ##

    ## try all the k = (1,...,n-1) where n is the number of observations
    max_k = nr_samples - 1

    ## Fill in the predicted class into the following matrix
    ## For each sample in the LOOCV one column, for each parameter 'k' of KNN one row
    predict_mat = matrix(nrow=max_k, ncol=nr_samples)

    ## MSE values for each k
    mse_vec = c()
    
    datatmp = data.sc
    
    for(k in 1:max_k) {
        
        ## Do LOOCV: take a test sample x_i and a training set (x_1, ..., x_i-1, x_i+1, ..., x_n).
        ## Make prediction with test sample x_i and calculate test MSE_i (BS)
        for(i in 1:nr_samples){
            
            train    = datatmp[-i, -1]
            train_cl = datatmp[-i, 1]
            
            test     = datatmp[i,-1]
            
            predict.model <- knn(train = train, test = test, cl = train_cl, k = k, prob = TRUE, use.all = TRUE)
            
            ## fill in the predicted class
            predict_mat[k,i] = unclass(predict.model) - 1

            
        }
        
        mse = 1/nr_samples *  sum((predict_mat[k,] - heart_data$Type)^2)
        mse_vec = c(mse_vec, mse)
        
    }
    

    ## find index that has lowest MSE, is equal to 'k'
    best_k_mse = which(mse_vec == min(mse_vec))[1]

    ##
    ## plot parameter k against MSE
    ##
    pdf(paste(OUT_DIR, "/knn_mse_k_dataset", DATA_SETUP_TEST, ".pdf", sep=""), width=5, height=5, paper='special' )

    plot(mse_vec, type="o", lty=1, lwd=2, xlim=c(1,length(mse_vec)), xlab="k", ylab="mean squared error")

    abline(v = best_k_mse, lty = 4, col="black")
    title("K-Nearest Neighbours")
    text(best_k_mse + 3, 0, "selected k")
    dev.off()
    

    

    ##
    ## Calculate the sensitivities and specificities for each parameter k
    ##  and plot these against k
    ##

    sens_max = 0
    spec_max = 0

    sens_vec = c()
    spec_vec = c()

    sensspec_max = 0
    
    for(k in 1:nrow(predict_mat)) {

        senstmp = Sensitivity(true.classes, predict_mat[k,])
        spectmp = Specificity(true.classes, predict_mat[k,])

        sens_vec = c(sens_vec, senstmp)
        spec_vec = c(spec_vec, spectmp)

        if ( (senstmp + spectmp) > sensspec_max) {
            sensspec_max = senstmp + spectmp
            best_k = k
            sens_max = senstmp
            spec_max = spectmp
        }

    }

    ##
    ## Plot parameter k = (1,...,n-1) against sensitivity and specificity
    ##

    pdf(paste(OUT_DIR, "/knn_sens_spec_k_dataset", DATA_SETUP_TEST, ".pdf", sep=""), width=5, height=5, paper='special' )

    plot(sens_vec, type="l", lty=2, lwd=2, ylim=c(0,1), xlim=c(1,nrow(predict_mat)), xlab="", ylab="")
    par(new=T)
    plot(spec_vec, type="l", lty=3, lwd=2, ylim=c(0,1), xlim=c(1,nrow(predict_mat)), xlab="k", ylab="Sensitivity and Specificity")

    abline(v = best_k, lty = 4, col="black")
    title("K-Nearest Neighbours")
    text(best_k + 3, 0, "selected k")
    dev.off()

    
    return(best_k_mse)
    
}


main_KNN <- function(heart_data, k_param) {
                              

    results = list()

    ##
    ## Prepare data for following executions
    ##
    
    data.sc = data.frame(scale(heart_data))
    data.sc[,1] = heart_data$Type
    datatmp = data.sc
    
    nr_samples = nrow(data.sc)

    ## predicted class labels go into here
    predicted = c()

    
    ##
    ## Do LOOCV: take a test sample x_i and a training set (x_1, ..., x_i-1, x_i+1, ..., x_n).
    ##
    for(i in 1:nr_samples) {
            
        train    = datatmp[-i, -1]
        train_cl = datatmp[-i, 1]
        
        test     = datatmp[i,-1]
        
        predict.model <- knn(train = train, test = test, cl = train_cl, k = k_param, prob = FALSE, use.all = TRUE)
        
        ## fill in the predicted class
        predicted = c(predicted, unclass(predict.model) - 1)
        
    }

    return(predicted)
    
}



    
    
    


knn_var_bootstrap <- function(heart_data) {


    ##
    ## Prepare data for following executions
    ##
    
    data.sc = data.frame(scale(heart_data))
    data.sc[,1] = heart_data$Type
      
    nr_samples = nrow(data.sc)

    
    ##
    ## Variance for bootstrap version for KNN variance
    ##
    
        
    bootsens <- c()
    bootspec <- c()
    
    
    for(j in 1:100) {
        
        ## sample indices with replacement
        bootstrap <- sample(1:nr_samples, size = nr_samples, replace = TRUE)
        
        ## extract data excluding class information in 1st row
        sampledata <- data.sc[c(bootstrap), -1 ]

        ## class of the samples in the first row
        sample_type = data.sc[c(bootstrap),1]

        
        predicts <- c()
        specs <- 0
        sens <- 0

        for(i in 1:nr_samples){


            ## exclude sample 'i', this is the training set and its type
            trainingdata  = sampledata[-i,]
            training_type = sample_type[-i]


            ## run knn with training set and on test element
            knnfit <- knn(train = trainingdata, test = sampledata[i,], cl = training_type, k = 1, prob = FALSE, use.all = TRUE)

            ## save the fit
            predicts = c(predicts, knnfit)

            ## If the prediction is correct
            if(knnfit == sample_type[i]){

                ## true negative
                if(knnfit == 0){
                    specs = specs + 1 
                }

                ## true positive
                if(knnfit == 1){
                    sens = sens + 1
                }
            }
        }
        
        sensrate  = sens  / sum(sample_type == 1)
        specsrate = specs / sum(sample_type == 0)

        bootsens <- c(bootsens, sensrate)
        bootspec <- c(bootspec, specsrate)

        
    } # end of bootstrap

                                        #
                                        # Standard deviations of sensitivity and specificity
                                        #

    sd_sens = sd(bootsens) #0.2028232
    sd_spec = sd(bootspec) #0.0168263

    return(list(sd_sens = sd_sens, sd_spec = sd_spec))

        

}
